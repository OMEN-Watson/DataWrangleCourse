{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing pattern table example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "       marital_status  occupation  credit_card_number      \n",
      "14575               1           1                   1     0\n",
      "1430                0           0                   0     3\n",
      "810                 0           1                   1     1\n",
      "                 2240        1430                1430  5100\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This module implements the missing values table in Rattle, using Python\n",
    "\n",
    "@Author: Charini Nanayakkara\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "file_name = sys.argv[1]\n",
    "file_name='merged_dataset_Pro.csv'\n",
    "pattern_count_dict = {} # Dictionary to store patterns of row values (as in \n",
    "                        # rattle missing file table). The key is a pattern. The\n",
    "                        # value is a tuple where the first value indicates the\n",
    "                        # number of times a pattern occurs and the second value\n",
    "                        # tells the number of variables with missing values per\n",
    "                        # each pattern. \n",
    "table_value_list = [] # Table to contain missing value data as list of lists.\n",
    "                      # The first value in the sub list is the missing value count \n",
    "                      # per row. Second value is a tuple containing the pattern\n",
    "                      # and the missing variables per pattern.\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "# selected_columns = df[['occupation', 'salary', 'credit_card_number']]\n",
    "# selected_columns = df[['street_address', 'suburb', 'postcode']]\n",
    "\n",
    "selected_columns = df[[ 'marital_status', 'occupation', 'credit_card_number' ]]\n",
    "# selected_columns = df[[ 'marital_status', 'occupation', 'salary' ]]\n",
    "\n",
    "\n",
    "  # 'marital_status', 'salary', 'credit_card_number' 'occupation', 'salary', 'credit_card_number'\n",
    "# null_table = df.isnull()\n",
    "null_table = selected_columns.isnull()\n",
    "tot_missing_vals_per_var = null_table.sum(axis=0) # Total missing values per variable\n",
    "total_missing_vals = tot_missing_vals_per_var.sum() # Total missing values\n",
    "\n",
    "for index, row in null_table.iterrows():\n",
    "  count,var_no = pattern_count_dict.get(tuple(row),(0,0))\n",
    "  count += 1\n",
    "  var_no = sum(row) # True = 1 and False = 0. Therefore, null values are counted\n",
    "  pattern_count_dict[tuple(row)] = (count,var_no)\n",
    "  \n",
    "\n",
    "for key,value in pattern_count_dict.items():\n",
    "  table_value_list.append([value[0], [int(not item) for item in key] + [value[1]]])\n",
    "  \n",
    "table_value_list.sort(reverse=True)\n",
    "\n",
    "table_value_list.append(['',tot_missing_vals_per_var.tolist() + \\\n",
    "                         [total_missing_vals]])\n",
    "\n",
    "\n",
    "data = [item[1] for item in table_value_list] # Data to write to table\n",
    "columns_list = tot_missing_vals_per_var.index.tolist() + ['']\n",
    "index_list = [item[0] for item in table_value_list]\n",
    "# Create new pandas DataFrame \n",
    "new_df = pd.DataFrame(data, columns = columns_list, index = index_list) \n",
    "print('---------------')\n",
    "print (new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing pattern example pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'F'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m tempData\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet_address\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuburb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostcode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Split the pattern back into columns\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pattern_counts[tempData] \u001b[38;5;241m=\u001b[39m \u001b[43mpattern_counts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpattern\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Reorder columns\u001b[39;00m\n\u001b[0;32m     31\u001b[0m pattern_counts \u001b[38;5;241m=\u001b[39m pattern_counts[[tempData[\u001b[38;5;241m0\u001b[39m], tempData[\u001b[38;5;241m1\u001b[39m], tempData[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32me:\\PY\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\PY\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\PY\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32me:\\PY\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\PY\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     24\u001b[0m tempData\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet_address\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuburb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostcode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Split the pattern back into columns\u001b[39;00m\n\u001b[0;32m     26\u001b[0m pattern_counts[tempData] \u001b[38;5;241m=\u001b[39m pattern_counts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Reorder columns\u001b[39;00m\n\u001b[0;32m     31\u001b[0m pattern_counts \u001b[38;5;241m=\u001b[39m pattern_counts[[tempData[\u001b[38;5;241m0\u001b[39m], tempData[\u001b[38;5;241m1\u001b[39m], tempData[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'F'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "# data = {\n",
    "#     'A': [5, None, 6, 7, None],\n",
    "#     'B': [3, 4, None, 5, None],\n",
    "#     'C': [None, 2, 1, 3, None]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "file_name='data_wrangling_education_2024_u7568823.csv'\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "# Display the dataset\n",
    "# print(\"Dataset:\")\n",
    "# print(df)\n",
    "\n",
    "# Create a missing indicator DataFrame\n",
    "missing_indicator = df.isnull()\n",
    "missing_indicator['pattern'] = missing_indicator.apply(lambda row: ''.join(row.astype(str)), axis=1)\n",
    "\n",
    "# Count the frequency of each missing pattern\n",
    "pattern_counts = missing_indicator.groupby('pattern').size().reset_index(name='frequency')\n",
    "tempData=['street_address', 'suburb', 'postcode']\n",
    "# Split the pattern back into columns\n",
    "pattern_counts[tempData] = pattern_counts['pattern'].apply(\n",
    "    lambda x: pd.Series(list(map(int, x)))\n",
    ")\n",
    "\n",
    "# Reorder columns\n",
    "pattern_counts = pattern_counts[[tempData[0], tempData[1], tempData[2], 'frequency']]\n",
    "\n",
    "print(\"\\nMissing Pattern Table:\")\n",
    "print(pattern_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T3.1 a,education dataset pro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per attribute:\n",
      "salary                  3019\n",
      "middle_name             2058\n",
      "email                   1993\n",
      "phone                   1930\n",
      "credit_card_number      1709\n",
      "occupation              1709\n",
      "postcode                  21\n",
      "suburb                    21\n",
      "state                     21\n",
      "street_address            15\n",
      "years_of_experience        0\n",
      "education                  0\n",
      "rec_id                     0\n",
      "ssn                        0\n",
      "birth_date                 0\n",
      "current_age                0\n",
      "gender                     0\n",
      "last_name                  0\n",
      "first_name                 0\n",
      "employment_timestamp       0\n",
      "dtype: int64\n",
      "\n",
      "Top combinations of three attributes with the highest number of missing values:\n",
      "Attributes: ('occupation', 'salary', 'credit_card_number'), Number of records with missing values: 254\n",
      "Attributes: ('email', 'occupation', 'credit_card_number'), Number of records with missing values: 177\n",
      "Attributes: ('middle_name', 'occupation', 'credit_card_number'), Number of records with missing values: 161\n",
      "Attributes: ('phone', 'occupation', 'credit_card_number'), Number of records with missing values: 158\n",
      "Attributes: ('middle_name', 'phone', 'salary'), Number of records with missing values: 36\n",
      "Attributes: ('email', 'occupation', 'salary'), Number of records with missing values: 28\n",
      "Attributes: ('email', 'salary', 'credit_card_number'), Number of records with missing values: 28\n",
      "Attributes: ('middle_name', 'email', 'salary'), Number of records with missing values: 26\n",
      "Attributes: ('phone', 'email', 'salary'), Number of records with missing values: 23\n",
      "Attributes: ('phone', 'occupation', 'salary'), Number of records with missing values: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the employment dataset\n",
    "employment_dataset = pd.read_csv('data_wrangling_education_2024_u7568823.csv')\n",
    "\n",
    "# Get the list of attribute names\n",
    "attributes = [col for col in employment_dataset.columns if col not in ['rec_id', 'ssn']]\n",
    "\n",
    "# Calculate missing values per attribute\n",
    "missing_counts = employment_dataset.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values per attribute:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Initialize a dictionary to store counts of missing combinations\n",
    "combination_missing_counts = {}\n",
    "\n",
    "# Generate all combinations of three attributes\n",
    "three_attr_combinations = list(combinations(attributes, 3))\n",
    "\n",
    "# For each combination, count the number of records where all three are missing\n",
    "for comb in three_attr_combinations:\n",
    "    # Create a boolean mask where all three attributes are missing\n",
    "    mask = employment_dataset[list(comb)].isnull().all(axis=1)\n",
    "    count_missing = mask.sum()\n",
    "    \n",
    "    # Store the count if there's at least one record missing all three attributes\n",
    "    if count_missing > 0:\n",
    "        combination_missing_counts[comb] = count_missing\n",
    "\n",
    "# Sort combinations by missing counts in descending order\n",
    "sorted_combinations = sorted(combination_missing_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top combinations\n",
    "print(\"\\nTop combinations of three attributes with the highest number of missing values:\")\n",
    "for comb, count in sorted_combinations[:10]:  # Adjust the number as needed\n",
    "    print(f\"Attributes: {comb}, Number of records with missing values: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T3.1 b pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the merged dataset:\n",
      "['ssn', 'age_at_consultation', 'medicare_number', 'marital_status', 'height', 'weight', 'bmi', 'blood_pressure', 'cholesterol_level', 'smoking_status', 'clinical_notes', 'consultation_timestamp', 'current_age', 'education', 'occupation', 'salary', 'credit_card_number', 'years_of_experience', 'employment_timestamp', 'rec_id', 'first_name', 'middle_name', 'last_name', 'gender', 'birth_date', 'street_address', 'suburb', 'postcode', 'state', 'phone', 'email']\n",
      "\n",
      "Missing values per attribute:\n",
      "salary                    2510\n",
      "marital_status            2240\n",
      "occupation                1430\n",
      "credit_card_number        1430\n",
      "phone                      641\n",
      "email                      474\n",
      "middle_name                162\n",
      "postcode                     5\n",
      "bmi                          0\n",
      "rec_id                       0\n",
      "medicare_number              0\n",
      "state                        0\n",
      "suburb                       0\n",
      "street_address               0\n",
      "birth_date                   0\n",
      "gender                       0\n",
      "last_name                    0\n",
      "first_name                   0\n",
      "employment_timestamp         0\n",
      "blood_pressure               0\n",
      "years_of_experience          0\n",
      "height                       0\n",
      "age_at_consultation          0\n",
      "weight                       0\n",
      "education                    0\n",
      "current_age                  0\n",
      "consultation_timestamp       0\n",
      "clinical_notes               0\n",
      "smoking_status               0\n",
      "cholesterol_level            0\n",
      "ssn                          0\n",
      "dtype: int64\n",
      "\n",
      "Top combinations of three attributes with the highest number of missing values:\n",
      "Attributes: ('marital_status', 'occupation', 'credit_card_number'), Number of records with missing values: 1430\n",
      "Attributes: ('marital_status', 'occupation', 'salary'), Number of records with missing values: 216\n",
      "Attributes: ('marital_status', 'salary', 'credit_card_number'), Number of records with missing values: 216\n",
      "Attributes: ('occupation', 'salary', 'credit_card_number'), Number of records with missing values: 216\n",
      "Attributes: ('marital_status', 'occupation', 'phone'), Number of records with missing values: 54\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the datasets\n",
    "medical_dataset = pd.read_csv('data_wrangling_medical_2024_u7568823.csv')\n",
    "employment_dataset = pd.read_csv('data_wrangling_education_2024_u7568823.csv')\n",
    "\n",
    "# Ensure the 'ssn' columns are strings and strip any leading/trailing whitespaces\n",
    "medical_dataset['ssn'] = medical_dataset['ssn'].astype(str).str.strip()\n",
    "employment_dataset['ssn'] = employment_dataset['ssn'].astype(str).str.strip()\n",
    "\n",
    "# Remove rows with missing SSN\n",
    "medical_dataset = medical_dataset.dropna(subset=['ssn'])\n",
    "employment_dataset = employment_dataset.dropna(subset=['ssn'])\n",
    "\n",
    "# Perform an inner join on 'ssn'\n",
    "merged_dataset = pd.merge(\n",
    "    medical_dataset, employment_dataset, on='ssn', how='inner', suffixes=('_medical', '_employment')\n",
    ")\n",
    "\n",
    "# Identify common attributes (excluding 'ssn')\n",
    "common_attributes = [col for col in medical_dataset.columns if col in employment_dataset.columns and col != 'ssn']\n",
    "\n",
    "# For each common attribute, combine the values\n",
    "for attr in common_attributes:\n",
    "    # Create a new column in merged_dataset\n",
    "    merged_dataset[attr] = merged_dataset[attr + '_medical'].combine_first(merged_dataset[attr + '_employment'])\n",
    "    \n",
    "    # Drop the original columns with suffixes\n",
    "    merged_dataset.drop(columns=[attr + '_medical', attr + '_employment'], inplace=True)\n",
    "\n",
    "# Include unique attributes from both datasets\n",
    "# Attributes unique to the medical dataset\n",
    "medical_unique_attrs = [col for col in medical_dataset.columns if col not in employment_dataset.columns and col != 'ssn']\n",
    "# Attributes unique to the employment dataset\n",
    "employment_unique_attrs = [col for col in employment_dataset.columns if col not in medical_dataset.columns and col != 'ssn']\n",
    "\n",
    "# Add unique attributes to the merged dataset\n",
    "# For medical unique attributes\n",
    "for attr in medical_unique_attrs:\n",
    "    merged_dataset[attr] = merged_dataset[attr + '_medical'] if attr + '_medical' in merged_dataset.columns else merged_dataset[attr]\n",
    "    if attr + '_medical' in merged_dataset.columns:\n",
    "        merged_dataset.drop(columns=[attr + '_medical'], inplace=True)\n",
    "\n",
    "# For employment unique attributes\n",
    "for attr in employment_unique_attrs:\n",
    "    merged_dataset[attr] = merged_dataset[attr + '_employment'] if attr + '_employment' in merged_dataset.columns else merged_dataset[attr]\n",
    "    if attr + '_employment' in merged_dataset.columns:\n",
    "        merged_dataset.drop(columns=[attr + '_employment'], inplace=True)\n",
    "\n",
    "# Check the columns in the merged dataset\n",
    "print(\"Columns in the merged dataset:\")\n",
    "print(merged_dataset.columns.tolist())\n",
    "\n",
    "# Calculate missing values per attribute\n",
    "missing_counts = merged_dataset.isnull().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values per attribute:\")\n",
    "print(missing_counts)\n",
    "# Save the merged dataset to a CSV file before deduplication\n",
    "merged_dataset.to_csv('merged_dataset_Pro.csv', index=False)\n",
    "from itertools import combinations\n",
    "\n",
    "# Get the list of attribute names for analysis (excluding 'ssn')\n",
    "attributes = [col for col in merged_dataset.columns if col != 'ssn']\n",
    "\n",
    "# Initialize a dictionary to store counts of missing combinations\n",
    "combination_missing_counts = {}\n",
    "\n",
    "# Generate all combinations of three attributes\n",
    "three_attr_combinations = list(combinations(attributes, 3))\n",
    "\n",
    "# For each combination, count the number of records where all three are missing\n",
    "for comb in three_attr_combinations:\n",
    "    # Create a boolean mask where all three attributes are missing\n",
    "    # [ 'marital_status', 'occupation', 'credit_card_number' ]\n",
    "    if ('marital_status' in list(comb) and 'occupation' in list(comb) and 'credit_card_number' in list(comb) ):\n",
    "         ii=2\n",
    "    mask = merged_dataset[list(comb)].isnull().all(axis=1)\n",
    "    count_missing = mask.sum()\n",
    "    \n",
    "    # Store the count if there's at least one record missing all three attributes\n",
    "    if count_missing > 0:\n",
    "        combination_missing_counts[comb] = count_missing\n",
    "\n",
    "# Sort combinations by missing counts in descending order\n",
    "sorted_combinations = sorted(combination_missing_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top combinations\n",
    "print(\"\\nTop combinations of three attributes with the highest number of missing values:\")\n",
    "for comb, count in sorted_combinations[:5]:  # Adjust the number as needed\n",
    "    print(f\"Attributes: {comb}, Number of records with missing values: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 3.2  missing value statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per attribute:\n",
      "salary                    2510\n",
      "marital_status            2240\n",
      "occupation                1430\n",
      "credit_card_number        1430\n",
      "phone                      641\n",
      "email                      474\n",
      "middle_name                162\n",
      "postcode                     5\n",
      "bmi                          0\n",
      "rec_id                       0\n",
      "medicare_number              0\n",
      "state                        0\n",
      "suburb                       0\n",
      "street_address               0\n",
      "birth_date                   0\n",
      "gender                       0\n",
      "last_name                    0\n",
      "first_name                   0\n",
      "employment_timestamp         0\n",
      "blood_pressure               0\n",
      "years_of_experience          0\n",
      "height                       0\n",
      "age_at_consultation          0\n",
      "weight                       0\n",
      "education                    0\n",
      "current_age                  0\n",
      "consultation_timestamp       0\n",
      "clinical_notes               0\n",
      "smoking_status               0\n",
      "cholesterol_level            0\n",
      "ssn                          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "merged_dataset = pd.read_csv('merged_dataset_Pro.csv')\n",
    "\n",
    "\n",
    "# Calculate missing values per attribute\n",
    "missing_counts = merged_dataset.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Display the missing counts\n",
    "print(\"Missing values per attribute:\")\n",
    "print(missing_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 3.2 missing value imputation salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary Prediction Model Evaluation:\n",
      "R-squared: 0.8087\n",
      "Mean Squared Error: 361169083.25\n",
      "\n",
      "Missing salary values have been imputed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the merged dataset\n",
    "merged_dataset = pd.read_csv('files/merged_dataset_correctWeight.csv')\n",
    "\n",
    "# Replace 'merged_dataset.csv' with your actual merged dataset file if needed\n",
    "# For this example, we assume 'merged_dataset' is already loaded as a DataFrame\n",
    "\n",
    "# Select features for the salary model\n",
    "# You may need to adjust the list of features based on your actual dataset\n",
    "features = [\n",
    "    'occupation',\n",
    "    'education',\n",
    "    'years_of_experience',\n",
    "    'current_age',  # Assuming 'current_age' is available\n",
    "    'gender',\n",
    "    'marital_status','height'\n",
    "    # Add more features if relevant\n",
    "]\n",
    "# marital_status_encoded   -0.073462\n",
    "# occupation_encoded       -0.082529\n",
    "# education_encoded        -0.427379\n",
    "# height                 0.335408\n",
    "# current_age            0.197198\n",
    "# years_of_experience    0.176602\n",
    "\n",
    "# Ensure these features are available in the dataset\n",
    "for feature in features:\n",
    "    if feature not in merged_dataset.columns:\n",
    "        print(f\"Feature '{feature}' is not in the dataset.\")\n",
    "        # You may choose to remove or impute this feature\n",
    "        features.remove(feature)\n",
    "\n",
    "# Prepare the data for modeling\n",
    "# Separate the data into two sets: one with known salary values and one with missing salary values\n",
    "# salary_known = merged_dataset[merged_dataset['salary'].notnull()]\n",
    "salary_known = merged_dataset[merged_dataset['salary']>0]\n",
    "\n",
    "# salary_missing = merged_dataset[merged_dataset['salary'].isnull()]\n",
    "salary_missing = merged_dataset[merged_dataset['salary'] <= 0]\n",
    "\n",
    "# Define X (predictors) and y (target) for the known salary data\n",
    "X_known = salary_known[features]\n",
    "y_known = salary_known['salary']\n",
    "\n",
    "# Define X to predict (with missing salary)\n",
    "X_missing = salary_missing[features]\n",
    "\n",
    "# Handle missing values in predictors\n",
    "# We'll use SimpleImputer to fill missing values in numerical features with mean and categorical features with most frequent\n",
    "numeric_features = X_known.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_known.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipelines for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Split the known data into training and testing sets for model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_known, y_known, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Salary Prediction Model Evaluation:\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# If the model performance is acceptable, proceed to predict missing salaries\n",
    "# Predict missing salary values\n",
    "salary_pred = model.predict(X_missing)\n",
    "\n",
    "# Assign predicted salaries to the missing salary entries \n",
    "# merged_dataset.loc[merged_dataset['salary'].isnull(), 'salary'] = salary_pred\n",
    "merged_dataset.loc[merged_dataset['salary']<= 0, 'salary'] = salary_pred\n",
    "print(\"\\nMissing salary values have been imputed.\")\n",
    "merged_dataset.to_csv('files/finalImputateSalary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 3.2 output salary/marital_status            missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing salary:\n",
      "0\n",
      "Data with missing salary has been saved as 'missing_salary_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# merged_data = pd.read_csv('merged_dataset_Pro.csv')\n",
    "merged_data = pd.read_csv('files/merged_dataset_Pro_maritalStatusAndSalary_imputation.csv')\n",
    "\n",
    "# Assuming 'merged_data' is already loaded and contains the salary column\n",
    "\n",
    "# Filter rows where 'salary' is missing (NaN)\n",
    "missing_salary_data = merged_data[merged_data['salary'].isnull()]\n",
    "# missing_salary_data = merged_data[merged_data['marital_status'].isnull()]\n",
    "\n",
    "\n",
    "# Output the rows with missing salary\n",
    "print(\"Rows with missing salary:\")\n",
    "# print(missing_salary_data)\n",
    "print(missing_salary_data.shape[0])\n",
    "# Optionally, save the filtered data with missing salary to a CSV file\n",
    "# missing_salary_data.to_csv('files/mergeData_Pro_missing_salary.csv', index=False)\n",
    "\n",
    "print(\"Data with missing salary has been saved as 'missing_salary_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing salary:\n",
      "              ssn  age_at_consultation      medicare_number marital_status  \\\n",
      "1      e141846696                   15   1387  33065  1  3             NaN   \n",
      "20     d179725160                   10   2447  18788  1  2             NaN   \n",
      "26     a134698894                   11   4076  94639  2  3             NaN   \n",
      "66     b185031612                    5   9293  18407  2  3             NaN   \n",
      "68     a132601276                   12   2736  73792  1  2             NaN   \n",
      "...           ...                  ...                  ...            ...   \n",
      "16790  h199331603                   11   9496  85428  1  2             NaN   \n",
      "16797  c155601802                    9   3390  82757  1  3             NaN   \n",
      "16802  d161523646                   10   8425  57673  1  2             NaN   \n",
      "16806  d104281639                    6   2533  52341  1  3             NaN   \n",
      "16808  d136644076                    9   2399  36590  1  3             NaN   \n",
      "\n",
      "       height  weight  bmi  blood_pressure  cholesterol_level  smoking_status  \\\n",
      "1         162      74   28              86                247               0   \n",
      "20        132      35   20              77                136               0   \n",
      "26        131      33   19              69                185               0   \n",
      "66        106      18   16              68                206               0   \n",
      "68        156      49   20              75                188               0   \n",
      "...       ...     ...  ...             ...                ...             ...   \n",
      "16790     133      47   26              72                137               0   \n",
      "16797     123      28   18              74                132               0   \n",
      "16802     147      34   15              81                226               0   \n",
      "16806     112      27   21              76                174               0   \n",
      "16808     153      45   19              68                166               0   \n",
      "\n",
      "       ... middle_name last_name  gender  birth_date  \\\n",
      "1      ...      foster    stultz       m  06/09/2004   \n",
      "20     ...     luquire   edwards       f  19/09/2006   \n",
      "26     ...       almid     sacra       m  25/08/2007   \n",
      "66     ...   sharmaine   carlson       f  19/09/2015   \n",
      "68     ...         NaN    scales       f  11/04/2004   \n",
      "...    ...         ...       ...     ...         ...   \n",
      "16790  ...     wellons   harrell       f  11/12/2003   \n",
      "16797  ...       marie      head       f  03/08/2007   \n",
      "16802  ...       joyce  cockrell       m  08/09/2010   \n",
      "16806  ...      foster     pitta       f  19/10/2014   \n",
      "16808  ...         ray   robbian       m  21/04/2002   \n",
      "\n",
      "                                street_address           suburb postcode  \\\n",
      "1                  533  north  road  longford          mortlake   3272.0   \n",
      "20             157  marconi  street  villa  9    bonnells  bay    2264.0   \n",
      "26       15  the  avenue  john  paul  nursing         heathcote   2233.0   \n",
      "66                 40  ananda  road  farewell     oyster  cove    7150.0   \n",
      "68      44  bungoona  road  broadwater  court         kincumber   2251.0   \n",
      "...                                        ...              ...      ...   \n",
      "16790            4  sturt  street  kilnacrott     wagga  wagga    2650.0   \n",
      "16797     75  broadwater  court  avoca  drive         kincumber   4217.0   \n",
      "16802             22  mitford  street  flt  2        st  kilda    3182.0   \n",
      "16806    32  schofield  road  schofield  hill            bywong   2621.0   \n",
      "16808             4  riversdale  road  flt  2          hawthorn   3122.0   \n",
      "\n",
      "       state             phone                   email  \n",
      "1        vic   03  6780  7652       john87@mail.com.au  \n",
      "20       nsw   02  5995  8680    loryn.edwards@aol.com  \n",
      "26       nsw   02  7500  5407   qaijmldski@mail.com.au  \n",
      "66       tas   03  6011  7678         carlson6@aol.com  \n",
      "68       nsw   07  3875  5035         scales87@aol.com  \n",
      "...      ...               ...                     ...  \n",
      "16790    nsw   02  6401  2384    harrell84@mail.com.au  \n",
      "16797    nsw   02  7894  2582       head38@mail.com.au  \n",
      "16802    vic   03  3220  4163     rtjdlggesk@gmail.com  \n",
      "16806    act   02  5196  3997        pitta54@gmail.com  \n",
      "16808    vic   02  2887  3248        robbian91.aol.com  \n",
      "\n",
      "[2240 rows x 31 columns]\n",
      "Data with missing salary has been saved as 'missing_salary_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_data = pd.read_csv('merged_dataset_Pro.csv')\n",
    "\n",
    "# Assuming 'merged_data' is already loaded and contains the salary column\n",
    "\n",
    "# Filter rows where 'salary' is missing (NaN)\n",
    "missing_salary_data = merged_data[merged_data['marital_status'].isnull()]\n",
    "\n",
    "# Output the rows with missing salary\n",
    "print(\"Rows with missing salary:\")\n",
    "print(missing_salary_data)\n",
    "\n",
    "# Optionally, save the filtered data with missing salary to a CSV file\n",
    "missing_salary_data.to_csv('files/mergeData_Pro_missing_marital_status.csv', index=False)\n",
    "\n",
    "print(\"Data with missing salary has been saved as 'missing_salary_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task3.2 marital imputation missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PY\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Marital Status Prediction Model Evaluation:\n",
      "Accuracy: 0.4271\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        divorced       0.00      0.00      0.00       158\n",
      "         married       0.43      0.99      0.60      1249\n",
      "married-de-facto       0.00      0.00      0.00       497\n",
      "     not-married       0.32      0.01      0.01       814\n",
      "       separated       0.00      0.00      0.00       144\n",
      "         widowed       0.00      0.00      0.00        53\n",
      "\n",
      "        accuracy                           0.43      2915\n",
      "       macro avg       0.12      0.17      0.10      2915\n",
      "    weighted avg       0.27      0.43      0.26      2915\n",
      "\n",
      "\n",
      "Missing marital_status values have been imputed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select features for the marital_status model\n",
    "# Adjust the features based on your dataset\n",
    "features = [\n",
    "    'current_age',  # Assuming 'current_age' is available\n",
    "    'gender',\n",
    "    'education',\n",
    "    'occupation',\n",
    "    # Add more features if relevant\n",
    "]\n",
    "\n",
    "merged_dataset = pd.read_csv('merged_dataset_Pro.csv')\n",
    "\n",
    "# Ensure these features are available in the dataset\n",
    "for feature in features:\n",
    "    if feature not in merged_dataset.columns:\n",
    "        print(f\"Feature '{feature}' is not in the dataset.\")\n",
    "        features.remove(feature)\n",
    "\n",
    "# Prepare the data for modeling\n",
    "# Separate the data into two sets: one with known marital_status values and one with missing marital_status values\n",
    "marital_known = merged_dataset[merged_dataset['marital_status'].notnull()]\n",
    "marital_missing = merged_dataset[merged_dataset['marital_status'].isnull()]\n",
    "\n",
    "# Define X (predictors) and y (target) for the known marital_status data\n",
    "X_known = marital_known[features]\n",
    "y_known = marital_known['marital_status']\n",
    "\n",
    "# Define X to predict (with missing marital_status)\n",
    "X_missing = marital_missing[features]\n",
    "\n",
    "# Handle missing values in predictors as before\n",
    "numeric_features = X_known.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_known.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and logistic regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "# Split the known data into training and testing sets for model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_known, y_known, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nMarital Status Prediction Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# If the model performance is acceptable, proceed to predict missing marital statuses\n",
    "# Predict missing marital_status values\n",
    "marital_pred = model.predict(X_missing)\n",
    "\n",
    "# Assign predicted marital statuses to the missing entries\n",
    "merged_dataset.loc[merged_dataset['marital_status'].isnull(), 'marital_status'] = marital_pred\n",
    "\n",
    "print(\"\\nMissing marital_status values have been imputed.\")\n",
    "merged_dataset.to_csv('files/merged_dataset_Pro_marital_status_imputation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task3.2 impute salary first and then impute marital  missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PY\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\PY\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Marital Status Prediction Model Evaluation:\n",
      "Accuracy: 0.4271\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        divorced       0.00      0.00      0.00       158\n",
      "         married       0.43      0.99      0.60      1249\n",
      "married-de-facto       0.00      0.00      0.00       497\n",
      "     not-married       0.32      0.01      0.01       814\n",
      "       separated       0.00      0.00      0.00       144\n",
      "         widowed       0.00      0.00      0.00        53\n",
      "\n",
      "        accuracy                           0.43      2915\n",
      "       macro avg       0.12      0.17      0.10      2915\n",
      "    weighted avg       0.27      0.43      0.26      2915\n",
      "\n",
      "\n",
      "Missing marital_status values have been imputed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select features for the marital_status model\n",
    "# Adjust the features based on your dataset\n",
    "features = [\n",
    "    'current_age',  # Assuming 'current_age' is available\n",
    "    'gender',\n",
    "    'education',\n",
    "    'occupation',\n",
    "    # Add more features if relevant\n",
    "]\n",
    "\n",
    "# merged_dataset = pd.read_csv('merged_dataset_Pro.csv')\n",
    "merged_dataset = pd.read_csv('files/merged_dataset_Pro_salary_imputation.csv')\n",
    "\n",
    "# Ensure these features are available in the dataset\n",
    "for feature in features:\n",
    "    if feature not in merged_dataset.columns:\n",
    "        print(f\"Feature '{feature}' is not in the dataset.\")\n",
    "        features.remove(feature)\n",
    "\n",
    "# Prepare the data for modeling\n",
    "# Separate the data into two sets: one with known marital_status values and one with missing marital_status values\n",
    "marital_known = merged_dataset[merged_dataset['marital_status'].notnull()]\n",
    "marital_missing = merged_dataset[merged_dataset['marital_status'].isnull()]\n",
    "\n",
    "# Define X (predictors) and y (target) for the known marital_status data\n",
    "X_known = marital_known[features]\n",
    "y_known = marital_known['marital_status']\n",
    "\n",
    "# Define X to predict (with missing marital_status)\n",
    "X_missing = marital_missing[features]\n",
    "\n",
    "# Handle missing values in predictors as before\n",
    "numeric_features = X_known.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_known.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and logistic regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "# Split the known data into training and testing sets for model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_known, y_known, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nMarital Status Prediction Model Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# If the model performance is acceptable, proceed to predict missing marital statuses\n",
    "# Predict missing marital_status values\n",
    "marital_pred = model.predict(X_missing)\n",
    "\n",
    "# Assign predicted marital statuses to the missing entries\n",
    "merged_dataset.loc[merged_dataset['marital_status'].isnull(), 'marital_status'] = marital_pred\n",
    "\n",
    "print(\"\\nMissing marital_status values have been imputed.\")\n",
    "merged_dataset.to_csv('files/merged_dataset_Pro_maritalStatusAndSalary_imputation.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts of Incorrect or Impossible Values per Attribute:\n",
      "age_at_consultation: 0 incorrect or impossible values\n",
      "current_age: 0 incorrect or impossible values\n",
      "height: 0 incorrect or impossible values\n",
      "weight: 1648 incorrect or impossible values\n",
      "bmi: 0 incorrect or impossible values\n",
      "blood_pressure: 16815 incorrect or impossible values\n",
      "cholesterol_level: 1326 incorrect or impossible values\n",
      "birth_date: 10194 incorrect or impossible values\n",
      "gender: 0 incorrect or impossible values\n",
      "email: 1822 incorrect or impossible values\n",
      "phone: 0 incorrect or impossible values\n",
      "credit_card_number: 13815 incorrect or impossible values\n",
      "salary: 2707 incorrect or impossible values\n",
      "years_of_experience: 0 incorrect or impossible values\n",
      "postcode: 16810 incorrect or impossible values\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Assuming 'merged_dataset' is your merged DataFrame\n",
    "merged_data = pd.read_csv('files/merged_dataset_Pro_maritalStatusAndSalary_imputation.csv')\n",
    "\n",
    "# Dictionary to store counts of incorrect values per attribute\n",
    "incorrect_values_counts = {}\n",
    "\n",
    "### A. age_at_consultation and current_age ###\n",
    "\n",
    "# Define maximum plausible age\n",
    "max_age = 120\n",
    "min_age = 0\n",
    "\n",
    "# Check 'age_at_consultation'\n",
    "invalid_age_at_consultation = merged_dataset[\n",
    "    (merged_dataset['age_at_consultation'] < min_age) |\n",
    "    (merged_dataset['age_at_consultation'] > max_age) |\n",
    "    (merged_dataset['age_at_consultation'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_age_at_consultation = len(invalid_age_at_consultation)\n",
    "incorrect_values_counts['age_at_consultation'] = count_invalid_age_at_consultation\n",
    "\n",
    "# Check 'current_age'\n",
    "invalid_current_age = merged_dataset[\n",
    "    (merged_dataset['current_age'] < min_age) |\n",
    "    (merged_dataset['current_age'] > max_age) |\n",
    "    (merged_dataset['current_age'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_current_age = len(invalid_current_age)\n",
    "incorrect_values_counts['current_age'] = count_invalid_current_age\n",
    "\n",
    "### B. height ###\n",
    "\n",
    "# Define plausible height range in cm\n",
    "min_height_cm = 50\n",
    "max_height_cm = 272\n",
    "\n",
    "invalid_height = merged_dataset[\n",
    "    (merged_dataset['height'] < min_height_cm) |\n",
    "    (merged_dataset['height'] > max_height_cm) |\n",
    "    (merged_dataset['height'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_height = len(invalid_height)\n",
    "incorrect_values_counts['height'] = count_invalid_height\n",
    "\n",
    "### C. weight ###\n",
    "\n",
    "# Define plausible weight range in kg\n",
    "min_weight_kg = 2\n",
    "max_weight_kg = 635\n",
    "\n",
    "invalid_weight = merged_dataset[\n",
    "    (merged_dataset['weight'] < min_weight_kg) |\n",
    "    (merged_dataset['weight'] > max_weight_kg) |\n",
    "    (merged_dataset['weight'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_weight = len(invalid_weight)\n",
    "incorrect_values_counts['weight'] = count_invalid_weight\n",
    "\n",
    "### D. bmi ###\n",
    "\n",
    "# Define plausible BMI range\n",
    "min_bmi = 10\n",
    "max_bmi = 100\n",
    "\n",
    "invalid_bmi = merged_dataset[\n",
    "    (merged_dataset['bmi'] < min_bmi) |\n",
    "    (merged_dataset['bmi'] > max_bmi) |\n",
    "    (merged_dataset['bmi'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_bmi = len(invalid_bmi)\n",
    "incorrect_values_counts['bmi'] = count_invalid_bmi\n",
    "\n",
    "### E. blood_pressure ###\n",
    "\n",
    "# Function to validate blood pressure format and values\n",
    "def validate_blood_pressure(bp_value):\n",
    "    if pd.isnull(bp_value):\n",
    "        return False\n",
    "    match = re.match(r'^(\\d{2,3})/(\\d{2,3})$', str(bp_value))\n",
    "    if match:\n",
    "        systolic = int(match.group(1))\n",
    "        diastolic = int(match.group(2))\n",
    "        # Plausible systolic: 70-250 mmHg\n",
    "        # Plausible diastolic: 40-150 mmHg\n",
    "        if 70 <= systolic <= 250 and 40 <= diastolic <= 150:\n",
    "            return False  # Value is valid\n",
    "    return True  # Value is invalid\n",
    "\n",
    "invalid_blood_pressure_mask = merged_dataset['blood_pressure'].apply(validate_blood_pressure)\n",
    "invalid_blood_pressure = merged_dataset[invalid_blood_pressure_mask]\n",
    "\n",
    "count_invalid_blood_pressure = len(invalid_blood_pressure)\n",
    "incorrect_values_counts['blood_pressure'] = count_invalid_blood_pressure\n",
    "\n",
    "### F. cholesterol_level ###\n",
    "\n",
    "# Define plausible cholesterol levels in mg/dL\n",
    "min_cholesterol = 100\n",
    "max_cholesterol = 400\n",
    "\n",
    "invalid_cholesterol = merged_dataset[\n",
    "    (merged_dataset['cholesterol_level'] < min_cholesterol) |\n",
    "    (merged_dataset['cholesterol_level'] > max_cholesterol) |\n",
    "    (merged_dataset['cholesterol_level'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_cholesterol = len(invalid_cholesterol)\n",
    "incorrect_values_counts['cholesterol_level'] = count_invalid_cholesterol\n",
    "\n",
    "### G. birth_date ###\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Assume current date\n",
    "current_date = pd.to_datetime('today')\n",
    "\n",
    "# Convert 'birth_date' to datetime if not already\n",
    "merged_dataset['birth_date'] = pd.to_datetime(merged_dataset['birth_date'], errors='coerce')\n",
    "\n",
    "invalid_birth_date = merged_dataset[\n",
    "    (merged_dataset['birth_date'] > current_date) |\n",
    "    (merged_dataset['birth_date'].isnull()) |\n",
    "    ((current_date - merged_dataset['birth_date']).dt.days / 365 > max_age)\n",
    "]\n",
    "\n",
    "count_invalid_birth_date = len(invalid_birth_date)\n",
    "incorrect_values_counts['birth_date'] = count_invalid_birth_date\n",
    "\n",
    "### H. gender ###\n",
    "\n",
    "valid_genders = ['M', 'F', 'Male', 'Female', 'm', 'f', 'male', 'female']\n",
    "\n",
    "invalid_gender = merged_dataset[\n",
    "    ~merged_dataset['gender'].str.strip().str.lower().isin(['m', 'f', 'male', 'female'])\n",
    "]\n",
    "\n",
    "count_invalid_gender = len(invalid_gender)\n",
    "incorrect_values_counts['gender'] = count_invalid_gender\n",
    "\n",
    "### I. email ###\n",
    "\n",
    "# Function to validate email\n",
    "def validate_email(email):\n",
    "    if pd.isnull(email):\n",
    "        return False\n",
    "    # Simple regex for email validation\n",
    "    pattern = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
    "    return not re.match(pattern, email)\n",
    "\n",
    "invalid_email_mask = merged_dataset['email'].apply(validate_email)\n",
    "invalid_email = merged_dataset[invalid_email_mask]\n",
    "\n",
    "count_invalid_email = len(invalid_email)\n",
    "incorrect_values_counts['email'] = count_invalid_email\n",
    "\n",
    "### J. phone ###\n",
    "\n",
    "# Function to validate phone number (simple check)\n",
    "def validate_phone(phone):\n",
    "    if pd.isnull(phone):\n",
    "        return False\n",
    "    # Remove allowed symbols and check if remaining characters are digits\n",
    "    allowed_symbols = ['+', '-', '(', ')', ' ']\n",
    "    phone_cleaned = ''.join(c for c in str(phone) if c not in allowed_symbols)\n",
    "    return not phone_cleaned.isdigit()\n",
    "\n",
    "invalid_phone_mask = merged_dataset['phone'].apply(validate_phone)\n",
    "invalid_phone = merged_dataset[invalid_phone_mask]\n",
    "\n",
    "count_invalid_phone = len(invalid_phone)\n",
    "incorrect_values_counts['phone'] = count_invalid_phone\n",
    "\n",
    "### K. credit_card_number ###\n",
    "\n",
    "# Function to validate credit card number using Luhn algorithm\n",
    "def validate_credit_card(number):\n",
    "    if pd.isnull(number):\n",
    "        return False\n",
    "    number = str(number).replace(' ', '')\n",
    "    if not number.isdigit():\n",
    "        return True\n",
    "    total = 0\n",
    "    reverse_digits = number[::-1]\n",
    "    for i, digit in enumerate(reverse_digits):\n",
    "        n = int(digit)\n",
    "        if i % 2 == 1:  # Even index in reversed string\n",
    "            n *= 2\n",
    "            if n > 9:\n",
    "                n -= 9\n",
    "        total += n\n",
    "    return total % 10 != 0\n",
    "\n",
    "invalid_credit_card_mask = merged_dataset['credit_card_number'].apply(validate_credit_card)\n",
    "invalid_credit_card = merged_dataset[invalid_credit_card_mask]\n",
    "\n",
    "count_invalid_credit_card = len(invalid_credit_card)\n",
    "incorrect_values_counts['credit_card_number'] = count_invalid_credit_card\n",
    "\n",
    "### L. salary ###\n",
    "\n",
    "# Define plausible salary range\n",
    "min_salary = 0\n",
    "max_salary = 1_000_000_000  # Adjust as appropriate\n",
    "\n",
    "invalid_salary = merged_dataset[\n",
    "    (merged_dataset['salary'] < min_salary) |\n",
    "    (merged_dataset['salary'] > max_salary) |\n",
    "    (merged_dataset['salary'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_salary = len(invalid_salary)\n",
    "incorrect_values_counts['salary'] = count_invalid_salary\n",
    "\n",
    "### M. years_of_experience ###\n",
    "\n",
    "# Define plausible range\n",
    "min_experience = 0\n",
    "max_experience = 70  # Assuming maximum working lifespan\n",
    "\n",
    "invalid_experience = merged_dataset[\n",
    "    (merged_dataset['years_of_experience'] < min_experience) |\n",
    "    (merged_dataset['years_of_experience'] > max_experience) |\n",
    "    (merged_dataset['years_of_experience'].isnull())\n",
    "]\n",
    "\n",
    "count_invalid_experience = len(invalid_experience)\n",
    "incorrect_values_counts['years_of_experience'] = count_invalid_experience\n",
    "\n",
    "### N. postcode ###\n",
    "\n",
    "# Function to validate postcode (example for Australian postcodes)\n",
    "def validate_postcode(postcode):\n",
    "    if pd.isnull(postcode):\n",
    "        return False\n",
    "    return not re.match(r'^\\d{4}$', str(postcode))\n",
    "\n",
    "invalid_postcode_mask = merged_dataset['postcode'].apply(validate_postcode)\n",
    "invalid_postcode = merged_dataset[invalid_postcode_mask]\n",
    "\n",
    "count_invalid_postcode = len(invalid_postcode)\n",
    "incorrect_values_counts['postcode'] = count_invalid_postcode\n",
    "\n",
    "### Output the counts of incorrect values ###\n",
    "\n",
    "print(\"\\nCounts of Incorrect or Impossible Values per Attribute:\")\n",
    "for attr, count in incorrect_values_counts.items():\n",
    "    print(f\"{attr}: {count} incorrect or impossible values\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
