Your ANU ID check code is:           4178ed74
Your student data set check code is: 07f7d940952b

  * Check this pair of codes is in the list provided on Wattle
  * If not contact the course convenor.

####  4178ed74 / 07f7d940952b


A2

Task1

1.1
Final Result:0.57

steps:
The Dice coefficient similarity based on unigrams (q = 1) between the strings 1237568 and 1238823 is 0.57

    Unigrams:
        String A (1237568): {1, 2, 3, 7, 5, 6, 8}
        String B (1238823): {1, 2, 3, 8, 8, 2, 3}

    Common Unigrams: 1, 2, 3, 8 — counted once in each.
        ∣A∩B∣=4∣A∩B∣=4

    Total Length: ∣A∣=7∣A∣=7, ∣B∣=7∣B∣=7

    Dice Coefficient:
    Dice(A,B)= 2*4/(7+7)=0.57

  1.2

  0.22
  steps:

    Bigrams:
        String A (1237568): {12, 23, 37, 75, 56, 68}
        String B (1238823): {12, 23, 38, 88, 82, 23}

    Common Bigrams (∣A∩B∣∣A∩B∣): 12, 23
        ∣A∩B∣=2 

    Unique Bigrams (∣A∪B∣∣A∪B∣): {12, 23, 37, 75, 56, 68, 38, 88, 82}
        ∣A∪B∣=9 

    Jaccard Similarity:
    Jaccard(A,B)=2/9=0.22
 

 1.3

 0.57


 steps:
 String X (1237568)  
String Y (1238823)  
Bag Distance:

    X−Y="756"X−Y="756"
    Y−X="238"Y−X="238"
    Bag Distance = 3

Length of Strings:

    max⁡(len(X),len(Y))=7 
Bag Distance Similarity:
Bag Distance Similarity=1−3/7=0.5714

1.4 
Edit Distance:   6

   |1|2|3|8|8|2|3
 |0|1|2|3|4|5|6|7
1|1|0|1|2|3|4|5|6
2|2|1|0|1|2|3|4|5
3|3|2|1|0|1|2|3|4
7|4|3|2|1|2|3|4|5
5|5|4|3|2|3|4|5|6
6|6|5|4|3|4|5|6|7
8|7|6|5|4|3|4|5|6

1.5

The bag distance measures the difference between two strings based solely on the count of each character, ignoring their order or position—treating the strings as unordered "bags" of characters. 
In contrast, the edit distance considers both character counts and their positions, so while the bag distance provides a lower bound, the edit distance gives a more comprehensive measure of the transformations needed to convert one string into another.

Task2

2.1

Number of unique SSNs occurred in common in both datasets: 16005
Data only in the medical dataset: 3995
Data only in the employment dataset: 3185


2.2


I performed an outer join to include all records from both the medical and employment datasets, ensuring no data was lost. Records unique to one dataset were included with `NaN` values for fields from the other dataset.

 Justification:
1. Data Completeness: Preserves all records for future analysis, even if they exist in only one dataset.
2. Real-World Scenarios: Reflects real situations where individuals may have medical or employment events, but not both.
3. Avoiding Bias: Ensures balanced representation by including individuals with only one type of event.




Task2.3


Number of unique SSNs with duplicate records in the medical dataset: 0
Those records in the employment dataset: 810

To handle duplicate records, I first identified all duplicate SSNs in both the medical and employment datasets. For deduplication, I consolidated these records by applying different strategies based on the type of data:

- Numerical attributes (e.g., BMI, salary) were handled by selecting the most recent value based on timestamps or averaging the values if appropriate.
- Categorical attributes (e.g., gender, education) were resolved by selecting the most frequent or consistent non-null entry.
- Textual attributes (e.g., clinical notes) were combined by concatenating all notes from duplicate records.

When conflicts arose, I prioritized the most accurate or complete records. After resolving conflicts, I merged duplicates into a single representative record for each SSN.

This approach ensured data integrity by preventing skewed analysis and redundancy, while retaining valuable information about each individual. 


    Task2.4

Inconsistency counts per attribute:
- first_name: 0 
- middle_name: 2801 
- last_name: 0 
- gender: 1631 
- birth_date: 0 
- street_address: 6597 
- suburb: 6490 
- postcode: 8358 
- state: 2677 
- phone: 8565 
- email: 6878 


To handle inconsistencies between the two datasets, I standardized text attributes (e.g., names, email, address) by converting them to lowercase and removing extra spaces. For names, I used a similarity threshold via the SequenceMatcher library, treating values as consistent if the similarity score was above 0.8. For gender, I mapped variations like "M", "F", "male", and "female" to standard values. Birth dates were handled with custom parsing to address potential formatting issues, such as "24:00" hours. Phone numbers and addresses were compared exactly. Inconsistencies were flagged, and counts were tracked for review.


Task3

Task3.1

for   (a) education data set:

Combination of attributes with the most missing values: ('occupation', 'salary', 'credit_card_number')
Number of records with missing values for this combination: 254

for  (b)   merged data set.

Combination :  ('marital_status', 'occupation', 'credit_card_number')
Number  : 1430



Task3 .2

Top 2 attributes with the highest number of missing values:
salary            2510
marital_status    2240

For those  attributes above in the merged dataset,I used the following imputation strategies based on correlation analysis:

 1. Salary:
   - Imputation Approach: Before imputing, I calculated the correlation between salary and other features (e.g., occupation, education) using Pearson correlation for numerical features and Label Encoding for categorical features. I selected features that showed strong positive or negative correlations with salary. I then used these features to build a Multiple Linear Regression model to predict missing salary values.
   - Justification: Salary, being a continuous variable, is well-suited for regression-based imputation. The features I selected, such as occupation, had a meaningful relationship with salary. By using these in the regression model, I ensured that the imputed values reflect the broader trends and dependencies present in the dataset, leading to more accurate predictions.

 2. Marital Status:

    Imputation Approach: I calculated correlations between marital_status and numerical features using Pearson correlation and evaluated associations with categorical features using Cramér’s V. The most relevant features (e.g., age, gender) were used to build a Multinomial Logistic Regression model for imputation.
    Justification: As a categorical variable, marital_status was imputed using logistic regression to predict category probabilities. This approach ensured realistic, data-driven imputation based on observed relationships.




Task3.3

   Counts of Incorrect or Impossible Values per Attribute:
weight: 1648 incorrect or impossible values
blood_pressure: 16815  
cholesterol_level: 1326 
birth_date: 10194
email: 1822
credit_card_number: 13815 
salary: 2707 
postcode: 16810 

Judgment standard：each attribute should be based on logical constraints and domain-specific validation rules. for example

Weight:  

 Flags weight values below 2 kg or above 635 kg, which are not plausible.
 
Birth Date:

    Ensures that birth_date is not in the future and is consistent with the calculated age.


Credit Card Number:

    Uses the Luhn algorithm to verify credit card numbers for validity.



 Task3.4

    for the incorrect or impossible values,i mainly take 3 steps:Standardization,Validation and Correction. for example:
    postcode attribute: I identified issues with invalid formats not matching the expected pattern (e.g., four digits for Australian postcodes).
    Actions Taken:

    Standardization: Trimmed leading/trailing spaces and converted to uppercase if letters were involved.
    Validation and Correction: Checked the format to ensure postcodes matched the four-digit pattern. Where errors like missing or transposed digits were found, I corrected them using available data such as suburb and state. For postcodes that could not be corrected, I marked them as NaN for further handling or exclusion.


    Task4 

    

 Task 4: Additional Data Exploration and Cleaning

To ensure the quality of the cleaned dataset, aimed at exploring the relationships between education, employment, and health, I conducted several additional data exploration and cleaning tasks. These tasks address key data quality dimensions and help prepare the dataset for analysis.

 1. Uniqueness and Duplicate Records:
   - Issue: Duplicate records may exist in the merged dataset, leading to skewed results in the analysis.
   - Action: I defined duplicates based on the same SSN and other key attributes. Using pandas’ `drop_duplicates()`, I removed these duplicates, ensuring that only the most complete or recent record for each individual was retained.
   - Justification: Removing duplicates ensures that each individual is uniquely represented, avoiding over-representation of specific individuals, which could bias the results.

 2. Data Transformation for Categorical Attributes:
   - Issue: Continuous BMI values need to be categorized to better analyze health patterns.
   - Action: I created derived variables by converting the continuous BMI values into categorical ranges (e.g., `Underweight`, `Normal weight`, `Overweight`, `Obese`), following the WHO classification.
   - Justification: This transformation allows for a clearer analysis of health trends by grouping individuals into meaningful BMI categories. It makes it easier to correlate health outcomes with education and employment factors.

 3. Outlier Detection and Treatment:
   - Issue: Some salary values were negative (e.g., -9999, -1280), which is unrealistic.
   - Action: I identified these erroneous salary values and imputed them using a predictive model based on relevant attributes such as `education`, `occupation`, and `years_of_experience`.
   - Justification: Imputing salary values based on predictors ensures the integrity of the data, maintaining realistic values and improving the dataset's accuracy for salary-related analysis.

 4. Other Problems Worth Correcting:
   - Email Validation: After merging the two datasets, some emails were in an invalid format (e.g., `123.gmail.com` instead of `123@gmail.com`). I noticed that when using the `combine_first()` method, incorrect email formats were retained over valid ones. Therefore, I validated the email addresses before merging, ensuring that the correct email (i.e., the one containing an "@" symbol) was kept.
   - Incorrect Weight Values: I found some weight values recorded as `-99`, which is clearly wrong. To address this, I calculated the correct weight using the available BMI and height values. This imputation ensured the weight values were consistent with the health metrics in the dataset.
   
 
